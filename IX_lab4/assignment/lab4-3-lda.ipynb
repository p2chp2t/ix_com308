{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Kenza Driss*\n",
    "* *Maximilien Hoffbeck*\n",
    "* *Jaeyi Jeong*\n",
    "* *Yoojin Kim*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/06 14:25:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/06 14:26:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/06/06 14:26:16 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 topics (with top words):\n",
      "Topic 1: theory, linear, problem, algorithm, concept, data, quantum, prerequisite, introduction, equation\n",
      "Topic 2: acoustic, audio, room, microphone, loudspeaker, processing, rotation, signal, hearing, training\n",
      "Topic 3: skill, teaching, assessment, end, outcome, technique, exercise, work, keywords, concept\n",
      "Topic 4: map, cartography, de, laba, landscape, territorial, la, journal, art, press\n",
      "Topic 5: energy, research, report, epfl, technology, laboratory, field, lab, semester, scientific\n",
      "Topic 6: drug, credit, risk, pharmacology, problem, theory, set, evaluate, target, assessment\n",
      "Topic 7: chemical, process, chemistry, protein, flow, water, transfer, concept, treatment, numerical\n",
      "Topic 8: material, protein, property, application, organic, presentation, fundamental, exercise, nanomaterials, physical\n",
      "Topic 9: electron, tem, liquid, microscopy, crystal, linear, 3d, reconstruction, lcd, sem\n",
      "Topic 10: chemical, biology, drug, biological, invited, semester, speaker, research, chemistry, talk\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LDA Course Topics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "with open(\"preprocessed_courses.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "local_data = [json.loads(line.strip()) for line in lines]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"courseId\", StringType(), True),\n",
    "    StructField(\"tokens\", ArrayType(StringType()), True)\n",
    "])\n",
    "df = spark.createDataFrame(local_data, schema=schema)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=10000, minDF=2.0)\n",
    "cv_model = cv.fit(df)\n",
    "result = cv_model.transform(df)\n",
    "\n",
    "k = 10\n",
    "lda = LDA(k=k, maxIter=10, seed=42, featuresCol=\"features\")\n",
    "lda_model = lda.fit(result)\n",
    "\n",
    "topics = lda_model.describeTopics()\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "def topic_terms(indices):\n",
    "    return [vocab[i] for i in indices]\n",
    "\n",
    "topics_list = topics.rdd.map(lambda row: topic_terms(row['termIndices'])).collect()\n",
    "\n",
    "print(\"Top 10 topics (with top words):\")\n",
    "for i, terms in enumerate(topics_list):\n",
    "    print(f\"Topic {i+1}: {', '.join(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print k = 10 topics extracted using LDA and give them labels.\n",
    "From the output, we can label the topics as following:\n",
    "- Topic 1: Theoretical Foundations & Algorithms\n",
    "- Topic 2: Acoustics & Audio Processing\n",
    "- Topic 3: Pedagogy & Skill Development\n",
    "- Topic 4: Urbanism & Territorial Planning\n",
    "- Topic 5: Energy & Scientific Research\n",
    "- Topic 6: Pharmacology & Risk Modeling\n",
    "- Topic 7: Chemical Engineering & Fluid Dynamics\n",
    "- Topic 8: Materials Science & Nanotech\n",
    "- Topic 9: Electron Microscopy & Imaging\n",
    "- Topic 10: Life Sciences & Scientific Seminars\n",
    "\n",
    "## How does it compare with LSI?\n",
    "In our analysis, both LSI and LDA were applied to the corpus to uncover latent topics. While LSI uses a linear algebraic approach based on SVD, LDA is a probabilistic generative model that assumes documents are mixtures of topics and topics are distributions over words.\n",
    "\n",
    "The topics extracted with LSI tended to be harder to interpret, often containing a mix of technical and generic terms from multiple domains. This made it difficult to assign clear, semantic labels to many of the latent dimensions identified by LSI.\n",
    "\n",
    "In contrast, LDA produced more coherent and interpretable topics. For example, some topics clearly aligned with specific domains such as “Acoustics and Audio Processing”, “Urbanism and Territorial Planning”, or “Pharmacology and Risk Modeling”. This thematic clarity made it significantly easier to label the topics and understand the underlying structure of the course offerings.\n",
    "\n",
    "While LSI can be useful for dimensionality reduction and reveals abstract concept spaces, LDA is better suited for topic interpretation and organization in textual datasets like course descriptions. The results suggest that LDA provides more actionable insights when the goal is to extract and label distinct themes from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varying α with fixed β = 1.01\n",
      "\n",
      "--- α = 1.1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: cell, material, process, presentation, assessment, data, concept, outcome, keywords, end\n",
      "Topic 2: circuit, laser, theory, time, quantum, power, introduction, noise, prerequisite, risk\n",
      "Topic 3: energy, concept, mass, end, principle, theory, assessment, outcome, teaching, week\n",
      "Topic 4: exercise, data, technique, prerequisite, assessment, introduction, end, keywords, control, linear\n",
      "Topic 5: process, report, end, research, teaching, outcome, assessment, reaction, concept, control\n",
      "Topic 6: skill, end, exercise, outcome, concept, report, work, teaching, activity, assessment\n",
      "Topic 7: problem, prerequisite, report, skill, data, application, work, assessment, solution, outcome\n",
      "Topic 8: process, material, theory, energy, structure, exercise, concept, application, end, prerequisite\n",
      "Topic 9: data, end, assessment, activity, policy, teaching, management, skill, outcome, class\n",
      "Topic 10: architecture, optic, optical, assessment, end, exercise, data, keywords, concept, outcome\n",
      "\n",
      "\n",
      "--- α = 2.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: cell, material, presentation, process, assessment, concept, data, outcome, end, keywords\n",
      "Topic 2: laser, circuit, time, theory, quantum, introduction, concept, prerequisite, noise, application\n",
      "Topic 3: energy, concept, mass, theory, end, assessment, outcome, principle, teaching, prerequisite\n",
      "Topic 4: exercise, data, prerequisite, assessment, technique, end, outcome, keywords, teaching, skill\n",
      "Topic 5: process, end, report, outcome, teaching, assessment, research, concept, work, application\n",
      "Topic 6: skill, end, concept, outcome, exercise, report, work, teaching, assessment, activity\n",
      "Topic 7: problem, prerequisite, data, skill, report, assessment, end, outcome, work, application\n",
      "Topic 8: process, material, theory, concept, energy, exercise, application, structure, end, prerequisite\n",
      "Topic 9: data, end, assessment, activity, policy, teaching, outcome, exercise, skill, treatment\n",
      "Topic 10: architecture, optic, optical, assessment, end, exercise, keywords, data, concept, outcome\n",
      "\n",
      "\n",
      "--- α = 5.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: cell, material, concept, assessment, end, presentation, outcome, process, data, keywords\n",
      "Topic 2: laser, concept, end, outcome, prerequisite, material, application, theory, assessment, time\n",
      "Topic 3: concept, energy, end, assessment, theory, outcome, teaching, exercise, prerequisite, keywords\n",
      "Topic 4: exercise, data, assessment, prerequisite, end, outcome, concept, skill, teaching, keywords\n",
      "Topic 5: process, end, outcome, assessment, teaching, report, concept, application, keywords, work\n",
      "Topic 6: skill, end, concept, assessment, outcome, work, exercise, teaching, report, activity\n",
      "Topic 7: problem, data, end, assessment, prerequisite, outcome, skill, work, concept, teaching\n",
      "Topic 8: material, concept, process, exercise, end, application, assessment, prerequisite, keywords, theory\n",
      "Topic 9: end, data, assessment, exercise, teaching, activity, outcome, skill, concept, process\n",
      "Topic 10: architecture, assessment, end, concept, keywords, exercise, outcome, prerequisite, teaching, data\n",
      "\n",
      "\n",
      "--- α = 10.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: concept, material, end, assessment, outcome, data, presentation, keywords, exercise, process\n",
      "Topic 2: concept, end, outcome, assessment, material, prerequisite, application, teaching, data, work\n",
      "Topic 3: concept, end, assessment, outcome, exercise, teaching, prerequisite, theory, keywords, energy\n",
      "Topic 4: exercise, assessment, data, concept, end, outcome, prerequisite, skill, teaching, keywords\n",
      "Topic 5: end, outcome, process, assessment, concept, teaching, exercise, application, keywords, prerequisite\n",
      "Topic 6: concept, end, assessment, outcome, skill, teaching, exercise, work, prerequisite, keywords\n",
      "Topic 7: end, assessment, outcome, data, prerequisite, concept, teaching, skill, exercise, work\n",
      "Topic 8: end, concept, material, exercise, assessment, outcome, application, process, prerequisite, keywords\n",
      "Topic 9: end, exercise, assessment, data, teaching, concept, outcome, skill, activity, prerequisite\n",
      "Topic 10: architecture, assessment, end, concept, outcome, exercise, keywords, teaching, prerequisite, data\n",
      "\n",
      "\n",
      "Varying β with fixed α = 6\n",
      "\n",
      "--- β = 1.1 ---\n",
      "Topic 1: material, concept, assessment, cell, end, outcome, data, presentation, process, keywords\n",
      "Topic 2: concept, end, material, outcome, assessment, prerequisite, application, laser, theory, teaching\n",
      "Topic 3: concept, end, energy, assessment, outcome, theory, teaching, exercise, prerequisite, keywords\n",
      "Topic 4: exercise, data, assessment, prerequisite, end, outcome, concept, teaching, skill, keywords\n",
      "Topic 5: end, process, outcome, assessment, teaching, concept, application, report, exercise, keywords\n",
      "Topic 6: end, concept, assessment, skill, outcome, teaching, exercise, work, prerequisite, activity\n",
      "Topic 7: end, assessment, data, outcome, problem, prerequisite, skill, concept, teaching, work\n",
      "Topic 8: material, concept, end, exercise, assessment, process, application, outcome, prerequisite, keywords\n",
      "Topic 9: end, assessment, data, exercise, teaching, outcome, concept, activity, skill, prerequisite\n",
      "Topic 10: assessment, architecture, end, concept, exercise, outcome, keywords, teaching, prerequisite, data\n",
      "\n",
      "\n",
      "--- β = 2.0 ---\n",
      "Topic 1: concept, end, assessment, outcome, material, data, exercise, keywords, process, prerequisite\n",
      "Topic 2: concept, end, assessment, outcome, material, prerequisite, teaching, exercise, application, work\n",
      "Topic 3: concept, end, assessment, outcome, teaching, exercise, prerequisite, keywords, theory, skill\n",
      "Topic 4: exercise, data, concept, assessment, end, outcome, prerequisite, teaching, keywords, skill\n",
      "Topic 5: end, outcome, assessment, process, concept, exercise, application, teaching, prerequisite, keywords\n",
      "Topic 6: end, concept, assessment, outcome, exercise, teaching, skill, prerequisite, work, keywords\n",
      "Topic 7: end, assessment, outcome, exercise, prerequisite, concept, teaching, skill, work, data\n",
      "Topic 8: end, concept, material, assessment, exercise, outcome, application, prerequisite, keywords, teaching\n",
      "Topic 9: end, assessment, data, concept, exercise, teaching, outcome, skill, prerequisite, activity\n",
      "Topic 10: concept, assessment, end, exercise, outcome, teaching, prerequisite, keywords, data, skill\n",
      "\n",
      "\n",
      "--- β = 5.0 ---\n",
      "Topic 1: concept, end, assessment, outcome, data, exercise, material, prerequisite, keywords, skill\n",
      "Topic 2: concept, end, assessment, outcome, prerequisite, exercise, teaching, material, data, keywords\n",
      "Topic 3: concept, end, assessment, outcome, exercise, teaching, prerequisite, keywords, skill, data\n",
      "Topic 4: concept, end, exercise, assessment, outcome, data, prerequisite, teaching, keywords, skill\n",
      "Topic 5: end, assessment, outcome, concept, exercise, teaching, prerequisite, application, keywords, process\n",
      "Topic 6: end, concept, assessment, outcome, exercise, teaching, prerequisite, skill, keywords, work\n",
      "Topic 7: end, assessment, outcome, concept, exercise, teaching, prerequisite, skill, data, work\n",
      "Topic 8: end, concept, assessment, exercise, outcome, material, prerequisite, keywords, teaching, application\n",
      "Topic 9: end, concept, assessment, outcome, exercise, teaching, data, prerequisite, skill, keywords\n",
      "Topic 10: concept, end, assessment, outcome, exercise, prerequisite, teaching, keywords, data, skill\n",
      "\n",
      "\n",
      "--- β = 10.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: end, concept, assessment, outcome, exercise, data, prerequisite, teaching, keywords, skill\n",
      "Topic 2: end, concept, assessment, outcome, exercise, prerequisite, teaching, data, keywords, material\n",
      "Topic 3: concept, end, assessment, outcome, exercise, teaching, prerequisite, keywords, skill, data\n",
      "Topic 4: end, concept, assessment, exercise, outcome, prerequisite, data, teaching, keywords, skill\n",
      "Topic 5: end, assessment, concept, outcome, exercise, teaching, prerequisite, keywords, application, data\n",
      "Topic 6: end, concept, assessment, outcome, exercise, teaching, prerequisite, skill, keywords, work\n",
      "Topic 7: end, assessment, concept, outcome, exercise, teaching, prerequisite, skill, data, keywords\n",
      "Topic 8: end, concept, assessment, outcome, exercise, prerequisite, teaching, keywords, material, application\n",
      "Topic 9: end, concept, assessment, outcome, exercise, teaching, prerequisite, data, skill, keywords\n",
      "Topic 10: concept, end, assessment, outcome, exercise, prerequisite, teaching, keywords, data, skill\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_lda(alpha, beta, k=10, max_iter=10, seed=42):\n",
    "    lda = LDA(k=k, maxIter=max_iter, seed=seed, optimizer=\"em\", \n",
    "              featuresCol=\"features\", docConcentration=[alpha], topicConcentration=beta)\n",
    "    model = lda.fit(result)\n",
    "    topics = model.describeTopics()\n",
    "    vocab = cv_model.vocabulary\n",
    "    return topics.rdd.map(lambda row: [vocab[i] for i in row['termIndices']]).collect()\n",
    "\n",
    "alphas = [1.1, 2.0, 5.0, 10.0]\n",
    "beta_fixed = 1.01\n",
    "print(\"Varying α with fixed β = 1.01\\n\")\n",
    "for alpha in alphas:\n",
    "    print(f\"--- α = {alpha} ---\")\n",
    "    topics_alpha = run_lda(alpha=alpha, beta=beta_fixed)\n",
    "    for i, terms in enumerate(topics_alpha):\n",
    "        print(f\"Topic {i+1}: {', '.join(terms)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "betas = [1.1, 2.0, 5.0, 10.0]\n",
    "alpha_fixed = 6\n",
    "print(\"Varying β with fixed α = 6\\n\")\n",
    "for beta in betas:\n",
    "    print(f\"--- β = {beta} ---\")\n",
    "    topics_beta = run_lda(alpha=alpha_fixed, beta=beta)\n",
    "    for i, terms in enumerate(topics_beta):\n",
    "        print(f\"Topic {i+1}: {', '.join(terms)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do these value impact the results?\n",
    "From the output, we notice that decreasing α results in more sharply defined topics, with each document dominated by only a few topics. This leads to clearer and more interpretable topics. Increasing α makes topics overlap more, introducing redundancy and reducing clarity.\n",
    "\n",
    "Similarly, reducing β yields more focused topics with distinct keywords, while increasing β smooths the topic-word distributions, causing topics to share many general terms. This results in less distinctive and less useful topics.\n",
    "\n",
    "It seems that lower values of α and β (around 1.1–2.0) produced the most coherent and informative topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: concept, end, exercise, prerequisite, assessment, teaching, theory, outcome, data, process\n",
      "Topic 2: audio, assessment, linear, processing, theory, introduction, technique, teaching, exercise, prerequisite\n",
      "Topic 3: processing, power, signal, application, energy, imaging, introduction, exercise, technique, keywords\n",
      "Topic 4: skill, exercise, ch, tool, research, concept, note, application, space, map\n",
      "Topic 5: research, report, scientific, work, presentation, skill, epfl, data, plan, laboratory\n",
      "Topic 6: credit, risk, assessment, information, evaluate, end, skill, application, introduction, form\n",
      "Topic 7: protein, numerical, flow, structure, concept, finite, fracture, element, problem, interaction\n",
      "Topic 8: exercise, skill, application, property, end, assessment, chemical, introduction, outcome, keywords\n",
      "Topic 9: linear, outcome, information, skill, theory, keywords, prerequisite, week, technique, activity\n",
      "Topic 10: drug, cell, end, prerequisite, teaching, introduction, keywords, concept, activity, protein\n"
     ]
    }
   ],
   "source": [
    "final_k = 10\n",
    "final_alpha = [1.1]\n",
    "final_beta = 0.6\n",
    "\n",
    "lda_final = LDA(k=final_k, maxIter=10, seed=42, optimizer=\"online\",\n",
    "                featuresCol=\"features\", docConcentration=final_alpha, topicConcentration=final_beta)\n",
    "lda_model_final = lda_final.fit(result)\n",
    "\n",
    "final_topics = lda_model_final.describeTopics()\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "def topic_terms(indices):\n",
    "    return [vocab[i] for i in indices]\n",
    "\n",
    "topics_words = final_topics.rdd.map(lambda row: topic_terms(row['termIndices'])).collect()\n",
    "\n",
    "# Print labeled topics\n",
    "for i, words in enumerate(topics_words):\n",
    "    print(f\"Topic {i+1}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain why you chose these values and write your labels for the topics.\n",
    "\n",
    "After experimenting with various hyperparameter settings, we selected the combination k = 10, α = 1.1, and β = 0.6 for its balance of topic coherence and coverage. We tried higher values for k, but reducing k minimized redundancy between topics, and a lower β value yielded tighter, more distinct word groupings within each topic.\n",
    "\n",
    "The resulting topics aligned well with major academic themes at EPFL, including Biotechnology, and Scientific Research Methods. The topics were both interpretable and diverse.\n",
    "\n",
    "We chose the following labels:\n",
    "- Introductory Concepts & Pedagogy\n",
    "- Audio Signal Processing\n",
    "- Signal & Imaging Systems\n",
    "- Spatial Analysis & Applied Skills\n",
    "- Research Methods & Scientific Work\n",
    "- Risk Analysis & Evaluation Systems\n",
    "- Structural Mechanics & Biomechanics\n",
    "- Materials & Chemical Properties\n",
    "- Mathematical Modeling & Methods\n",
    "- Life Sciences & Biotech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: nitrogen, star, wars, film, lucas, nitrate, lego, films, gunpowder, osaka\n",
      "Topic 2: test, bradman, cricket, court, pakistan, qin, india, england, sri, innings\n",
      "Topic 3: eagle, gruffydd, gwynedd, crocodile, water, llywelyn, ap, owain, crocodiles, nile\n",
      "Topic 4: alphabet, greek, letters, church, alphabets, south, bacon, part, somalia, malwa\n",
      "Topic 5: planet, star, stars, planets, acid, orbit, sun, comet, system, objects\n",
      "Topic 6: water, nuclear, high, gas, oil, energy, number, made, production, carbon\n",
      "Topic 7: apple, putin, intel, russian, computer, mac, macintosh, russia, software, computers\n",
      "Topic 8: aircraft, mk, narcissus, tintin, oxford, mosquito, fighter, air, squadron, phantom\n",
      "Topic 9: time, years, john, england, english, made, world, work, music, book\n",
      "Topic 10: space, moon, orbit, launch, mission, lunar, rocket, landing, spacecraft, earth\n",
      "Topic 11: open, wimbledon, tennis, deer, final, masters, won, murray, singles, rama\n",
      "Topic 12: shiva, boer, british, ganesha, boers, deities, mi, war, shankara, transvaal\n",
      "Topic 13: american, march, january, july, december, june, october, february, november, april\n",
      "Topic 14: church, orthodox, god, religious, christian, pope, century, roman, catholic, holy\n",
      "Topic 15: species, found, time, human, called, years, common, large, water, century\n",
      "Topic 16: amber, spiders, error, spider, unexpected, zeus, lava, eruption, instrument, species\n",
      "Topic 17: theory, number, system, field, called, mass, equation, form, function, point\n",
      "Topic 18: war, city, government, united, states, world, state, national, years, south\n",
      "Topic 19: dna, string, sequence, theory, panda, sequences, function, strings, alignment, number\n",
      "Topic 20: function, numbers, water, series, swastika, functions, element, number, real, point\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [t for t in tokens if re.match(\"^[a-zA-Z]{2,}$\", t)]\n",
    "    \n",
    "raw_rdd = spark.sparkContext.textFile(\"ix-data/wikipedia-for-schools.txt\")\n",
    "parsed_rdd = raw_rdd.map(lambda line: json.loads(line)).map(lambda d: (d[\"title\"], clean_tokens(d[\"tokens\"])))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"tokens\", ArrayType(StringType()), True)\n",
    "])\n",
    "wiki_df = spark.createDataFrame(parsed_rdd, schema=schema)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=20000, minDF=5.0)\n",
    "cv_model = cv.fit(wiki_df)\n",
    "vectorized_df = cv_model.transform(wiki_df)\n",
    "\n",
    "lda = LDA(k=20, maxIter=10, seed=42, optimizer=\"online\",\n",
    "          featuresCol=\"features\", docConcentration=[0.5], topicConcentration=0.3)\n",
    "lda_model = lda.fit(vectorized_df)\n",
    "\n",
    "topics = lda_model.describeTopics()\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "def map_terms(indices):\n",
    "    return [vocab[i] for i in indices]\n",
    "\n",
    "topics_words = topics.rdd.map(lambda row: map_terms(row[\"termIndices\"])).collect()\n",
    "\n",
    "for i, words in enumerate(topics_words):\n",
    "    print(f\"Topic {i+1}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the values for k, α and β that you chose a priori and why you picked them.\n",
    "We chose the following LDA hyperparameters for the Wikipedia-for-Schools dataset:\n",
    "- k = 20\n",
    "- α = 0.5\n",
    "- β = 0.3\n",
    "\n",
    "These values were selected to reflect the breadth and specificity of the Wikipedia content. A larger number of topics (k=20) accounts for the wide variety of domains covered. A lower α encourages focused topic distributions per article, and a lower β helps generate sharper, more coherent topics.\n",
    "\n",
    "## Are you convinced by the results? Give labels to the topics if possible. \n",
    "Yes, after filtering irrelevant tokens, the results are substantially better after, and many of the topics align clearly with recognizable themes. Here are the labels we chose:\n",
    "- Movies & Entertainment\n",
    "- Cricket & South Asian Sports\n",
    "- Welsh History & Geography\n",
    "- Languages & Alphabets\n",
    "- Astronomy & Space Objects\n",
    "- Energy & Natural Resources\n",
    "- Computing & Technology\n",
    "- Military Aircraft & Aviation\n",
    "- Literature & History\n",
    "- Space Exploration\n",
    "- Tennis & Sports Tournaments\n",
    "- Indian Deities & Colonial History\n",
    "- Calendar & Dates\n",
    "- Christianity & Religion\n",
    "- Biology & Species\n",
    "- Spiders, Natural Phenomena & Mythology\n",
    "- Mathematics & Physics\n",
    "- Politics & Global History\n",
    "- Genetics & Bioinformatics\n",
    "- Mathematics & Abstract Concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
