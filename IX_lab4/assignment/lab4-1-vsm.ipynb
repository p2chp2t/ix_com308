{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Kenza Driss*\n",
    "* *Maximilien Hoffbeck*\n",
    "* *Jaeyi Jeong*\n",
    "* *Yoojin Kim*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed terms for COM-308: ['20', '30', '300', '50', 'acquired', 'activity', 'ad', 'advertisement', 'algebra', 'algorithm', 'analytics', 'application', 'assessment', 'auction', 'balance', 'based', 'cathedra', 'chain', 'class', 'cloud', 'clustering', 'collection', 'combination', 'commerce', 'communication', 'community', 'computing', 'concept', 'concrete', 'contained', 'coverage', 'current', 'data', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'dimensionality', 'draw', 'effectiveness', 'efficiency', 'end', 'exam', 'expected', 'explore', 'explores', 'field', 'final', 'foundational', 'framework', 'function', 'fundamental', 'good', 'graph', 'hadoop', 'hand', 'homework', 'important', 'information', 'infrastructure', 'inspired', 'internet', 'java', 'key', 'keywords', 'knowledge', 'lab', 'laboratory', 'large', 'linear', 'machine', 'main', 'map', 'markov', 'material', 'medium', 'midterm', 'mining', 'modeling', 'network', 'networking', 'number', 'online', 'outcome', 'past', 'practical', 'practice', 'prerequisite', 'problem', 'provide', 'question', 'real', 'recommended', 'recommender', 'reduce', 'reduction', 'related', 'required', 'retrieval', 'scale', 'search', 'seek', 'service', 'session', 'social', 'spark', 'specifically', 'start', 'statistic', 'stochastic', 'stream', 'structure', 'teaching', 'technique', 'theoretical', 'theory', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'work', 'world']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text string into a list of lemmatized, clean tokens.\n",
    "\n",
    "    Steps performed :\n",
    "    1. Converts the text to lowercase.\n",
    "    2. Removes all punctuation and non-alphanumeric characters (retaining spaces).\n",
    "    3. Tokenizes the cleaned text into individual words using NLTK's word_tokenize.\n",
    "    4. Filters out stopwords and single-character tokens.\n",
    "    5. Lemmatizes each token to its base form (using WordNet Lemmatizer).\n",
    "\n",
    "    Args :\n",
    "        text (str) : The raw input text string to preprocess.\n",
    "\n",
    "    Returns :\n",
    "        List[str] : A list of processed, lemmatized tokens from the input text.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and single-char tokens\n",
    "    tokens = [t for t in tokens if t not in stopwords and len(t) > 1]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Build document-term lists for all courses\n",
    "doc_tokens = {}\n",
    "vocab_counter = Counter()\n",
    "for course in courses:\n",
    "    cid = course['courseId']\n",
    "    desc = course['description']\n",
    "    tokens = preprocess(desc)\n",
    "    doc_tokens[cid] = tokens\n",
    "    vocab_counter.update(tokens)\n",
    "\n",
    "# Filter out very frequent/infrequent terms\n",
    "min_df = 2  # term must appear in at least 2 documents\n",
    "max_df_ratio = 0.8  # term appears in less than 80% of docs\n",
    "N = len(courses)\n",
    "filtered_vocab = {term for term, df in vocab_counter.items()\n",
    "                  if df >= min_df and df <= max_df_ratio * N}\n",
    "\n",
    "# Prune tokens to filtered vocab\n",
    "for cid, tokens in doc_tokens.items():\n",
    "    doc_tokens[cid] = [t for t in tokens if t in filtered_vocab]\n",
    "\n",
    "# Save preprocessed content to a new file\n",
    "with open('preprocessed_courses.txt', 'w') as f:\n",
    "    for cid, tokens in doc_tokens.items():\n",
    "        # Write courseId and tokens as a JSON dictionary\n",
    "        course_data = {'courseId': cid, 'tokens': tokens}\n",
    "        f.write(json.dumps(course_data) + '\\n')\n",
    "\n",
    "# Print terms for COM-308 (Internet Analytics) in alphabetical order\n",
    "target = 'COM-308'\n",
    "terms_ix = sorted(set(doc_tokens[target]))\n",
    "print(\"Preprocessed terms for {}: {}\".format(target, terms_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1 Explanation : \n",
    "\n",
    "### We chose the following pre-processing steps :\n",
    "\n",
    "1. **Lowercasing**  \n",
    "   - **What :** Convert all text to lowercase.  \n",
    "   - **Why :** Ensures that “Internet” and “internet” are treated identically, reducing duplicate entries in the vocabulary.\n",
    "\n",
    "2. **Punctuation Removal**  \n",
    "   - **What :** Strip out any character that is not a letter, digit, or whitespace using `re.sub(r\"[^a-z0-9\\s]\", \" \", text)`.  \n",
    "   - **Why :** Punctuation (commas, periods, parentheses, etc.) generally does not carry semantic content in bag-of-words models and can be safely removed.\n",
    "\n",
    "3. **Regex-Based Tokenization**  \n",
    "   - **What :** Use `re.findall(r\"\\b[a-z0-9]+\\b\", text)` to extract contiguous alphanumeric tokens.  \n",
    "   - **Why :** Avoids external dependencies (NLTK’s Punkt tokenizer) and is sufficient for splitting on whitespace and punctuation.\n",
    "\n",
    "4. **Stopword Removal**  \n",
    "   - **What :** Discard tokens found in `stopwords.pkl`.  \n",
    "   - **Why :** Common English words (for example, “the”, “and”) appear in almost every document and add noise to similarity computations.\n",
    "\n",
    "5. **Single‐Character Token Removal**  \n",
    "   - **What :** Filter out any token of length ≤ 1.  \n",
    "   - **Why :** Single letters (for example, “a”, “I”) are either stopwords or not informative.\n",
    "\n",
    "6. **Lemmatization**  \n",
    "   - **What :** Apply NLTK’s `WordNetLemmatizer` to each token.  \n",
    "   - **Why :** Reduces inflected or derived forms to their dictionary root (for example, “computing” -> “compute”), consolidating features.\n",
    "\n",
    "7. **Document-Frequency Filtering**  \n",
    "   - **What :** Keep only terms that occur in at least `min_df=2` documents and in at most `max_df_ratio=0.8` of all documents.  \n",
    "   - **Why :**  \n",
    "     - Removing **infrequent** words (df < 2) reduces noise from typos or overly specialized terms.  \n",
    "     - Removing **very frequent** words (df / N > 0.8) filters out quasi-stopwords that escaped the stopword list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 terms in COM-308 by TF-IDF:\n",
      "mining                3.1137\n",
      "online                2.9099\n",
      "social                2.6387\n",
      "explore               2.4393\n",
      "world                 2.3989\n",
      "networking            2.2237\n",
      "hadoop                2.0189\n",
      "real                  1.9034\n",
      "recommender           1.8838\n",
      "service               1.8072\n",
      "commerce              1.7879\n",
      "auction               1.7879\n",
      "datasets              1.6527\n",
      "retrieval             1.6527\n",
      "ad                    1.6013\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Build term <-> index and doc <-> index mappings\n",
    "terms = sorted(filtered_vocab)                   # list of all retained vocabulary terms\n",
    "term2idx = {t: i for i, t in enumerate(terms)}   # map term -> row index\n",
    "doc2idx  = {cid: j for j, cid in enumerate(doc_tokens)}  # map courseId -> col index\n",
    "\n",
    "M = len(terms)   # number of rows (terms)\n",
    "N = len(doc_tokens)  # number of columns (documents)\n",
    "\n",
    "# Compute document frequencies df[t]\n",
    "df = Counter()\n",
    "for tokens in doc_tokens.values():\n",
    "    for t in set(tokens):\n",
    "        df[t] += 1\n",
    "\n",
    "# Compute idf[t] = log(N / df[t])\n",
    "idf = {t: np.log(N / df[t]) for t in terms}\n",
    "\n",
    "# Assemble sparse TF-IDF matrix X of shape (M, N) \n",
    "# We use max-normalized TF : tf_{t,d} = count(t in d) / max_t' count(t' in d)\n",
    "rows, cols, data = [], [], []\n",
    "for cid, tokens in doc_tokens.items():\n",
    "    j = doc2idx[cid]\n",
    "    tf_counter = Counter(tokens)\n",
    "    max_tf = max(tf_counter.values()) if tf_counter else 1\n",
    "    for t, tf in tf_counter.items():\n",
    "        i = term2idx[t]\n",
    "        tf_norm = tf / max_tf\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(tf_norm * idf[t])\n",
    "\n",
    "X = csr_matrix((data, (rows, cols)), shape=(M, N))\n",
    "\n",
    "# Extract and print top-15 terms for COM-308 by TF-IDF score\n",
    "target = 'COM-308'\n",
    "j = doc2idx[target]\n",
    "col = X[:, j].toarray().ravel()          # dense vector of TF-IDF scores for COM-308\n",
    "top15_idx = np.argsort(col)[-15:][::-1]  # indices of the 15 largest scores\n",
    "\n",
    "print(f\"Top 15 terms in {target} by TF-IDF:\")\n",
    "for idx in top15_idx:\n",
    "    term = terms[idx]\n",
    "    score = col[idx]\n",
    "    print(f\"{term:20s}  {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added for part 2\n",
    "from utils import save_pkl\n",
    "\n",
    "save_pkl(X, 'X.pkl')\n",
    "save_pkl(terms, 'terms.pkl')\n",
    "doc_ids = list(doc_tokens.keys())\n",
    "save_pkl(doc_ids, 'doc_ids.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2 Explanation : \n",
    "\n",
    "### Description :\n",
    "In this section, we compute the **TF-IDF matrix** from scratch for the pre-processed corpus of course descriptions. The matrix represents each document as a vector of term scores, enabling document similarity analysis later on.\n",
    "\n",
    "1. **Building Mappings**  \n",
    "   - **terms :** List of unique terms in the filtered vocabulary.\n",
    "   - **term2idx :** Maps each term to its row index in the matrix.  \n",
    "   - **doc2idx :** Maps each document (by `courseId`) to its column index in the matrix.\n",
    "\n",
    "2. **Calculating Document Frequencies (`df`)**  \n",
    "   - For each term, count in how many documents it appears. This gives the denominator for the IDF calculation.\n",
    "\n",
    "3. **Computing IDF (`idf`)**  \n",
    "   - For each term, compute the Inverse Document Frequency as :  \n",
    "      $[\n",
    "     \\text{idf}_t = \\log\\frac{N}{\\text{df}_t}\n",
    "     $]\n",
    "     where \\( N \\) is the total number of documents. Terms appearing in many documents will have a low IDF, while rare terms will have a high IDF.\n",
    "\n",
    "4. **Assembling the Sparse TF-IDF Matrix (`X`)**  \n",
    "   - For each document :  \n",
    "     - Count term frequencies (TF).  \n",
    "     - Normalize each term frequency by the document’s maximum term frequency.  \n",
    "     - Compute TF-IDF weight as $( \\text{tf}_{t,d} \\times \\text{idf}_t $).  \n",
    "     - Store non-zero entries as `(row, col, value)` for sparse construction.\n",
    "\n",
    "5. **Extracting Top Terms for COM-308 (Internet Analytics)**  \n",
    "   - Extract the column corresponding to COM-308.  \n",
    "   - Sort by descending TF-IDF value.  \n",
    "   - Print the top 15 terms along with their scores.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Answer to Explain where the difference between the large scores and the small ones comes from.\n",
    "The **difference** in scores is driven by two main factors :\n",
    "\n",
    "- **Term Frequency (TF) :**  \n",
    "  Terms like “mining”, “online”, and “social” appear frequently within the COM-308 description, increasing their TF weight.  \n",
    "- **Inverse Document Frequency (IDF) :**  \n",
    "  These terms are not common across the entire corpus of courses (low document frequency), which increases their IDF weight.  \n",
    "Thus, the **highest scores** correspond to terms that are both important in the COM-308 document and rare elsewhere, reflecting their specificity and relevance.  \n",
    "\n",
    "On the other hand :\n",
    "- **Low scores** (though not shown here) would correspond to terms that either appear rarely in COM-308 (low TF) or are very common across many courses (low IDF), or both. These terms contribute little to distinguishing COM-308 from other documents.\n",
    "\n",
    "This pattern is exactly what we expect in a **Vector Space Model** : unique, frequently used terms within a document (that aren’t common in the corpus) are key discriminators and receive higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 courses for query: \"markov chains\"\n",
      "MGT-484     Applied probability & stochastic processes     score=0.5701\n",
      "MATH-332    Applied stochastic processes                   score=0.5331\n",
      "COM-516     Markov chains and algorithmic applications     score=0.4491\n",
      "MGT-526     Supply chain management                        score=0.3694\n",
      "MGT-602     Mathematical models in supply chain management  score=0.2955\n",
      "\n",
      "Top 5 courses for query: \"facebook\"\n",
      "EE-727      Computational Social Media                     score=0.1836\n",
      "MSE-440     Composites technology                          score=0.0000\n",
      "BIO-695     Image Processing for Life Science              score=0.0000\n",
      "FIN-523     Global business environment                    score=0.0000\n",
      "MICRO-614   Electrochemical nano-bio-sensing and bio/CMOS interfaces  score=0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def make_query_vector(query, term2idx, idf, preprocess, M):\n",
    "    \"\"\"\n",
    "    Build a TF-IDF vector for an arbitrary query string.\n",
    "    - query: raw text, e.g. \"markov chains\"\n",
    "    - term2idx: mapping term -> row index in X\n",
    "    - idf: dict term -> idf value\n",
    "    - preprocess: function(text) -> list of tokens\n",
    "    - M: size of vocabulary\n",
    "    \"\"\"\n",
    "    tokens = preprocess(query)\n",
    "    vec = np.zeros(M)\n",
    "    tf_q = Counter(tokens)\n",
    "    max_tf = max(tf_q.values()) if tf_q else 1\n",
    "    for t, tf in tf_q.items():\n",
    "        if t in term2idx:\n",
    "            i = term2idx[t]\n",
    "            vec[i] = (tf / max_tf) * idf.get(t, 0.0)\n",
    "    return vec\n",
    "\n",
    "def top_k_similar(query, X, courses, term2idx, idf, preprocess, doc2idx, k=5):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between query and each document.\n",
    "    Return top-k as list of (courseId, courseName, score).\n",
    "    \"\"\"\n",
    "    M, N = X.shape\n",
    "    qv = make_query_vector(query, term2idx, idf, preprocess, M)\n",
    "    q_norm = norm(qv)\n",
    "    results = []\n",
    "    for course in courses:\n",
    "        cid = course['courseId']\n",
    "        j = doc2idx[cid]\n",
    "        dv = X[:, j].toarray().ravel()\n",
    "        d_norm = norm(dv)\n",
    "        if q_norm > 0 and d_norm > 0:\n",
    "            score = (qv @ dv) / (q_norm * d_norm)\n",
    "        else:\n",
    "            score = 0.0\n",
    "        results.append((cid, course['name'], score))\n",
    "    # sort descending by score\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:k]\n",
    "\n",
    "# Run searches and print\n",
    "for query in [\"markov chains\", \"facebook\"]:\n",
    "    print(f\"\\nTop 5 courses for query: \\\"{query}\\\"\")\n",
    "    for cid, name, score in top_k_similar(query, X, courses, term2idx, idf, preprocess, doc2idx, k=5):\n",
    "        print(f\"{cid:10s}  {name:45s}  score={score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3 Explanation : \n",
    "\n",
    "### Description :\n",
    "In this exercise, we are tasked with performing **document similarity search** using a **vector space model**. Specifically, we :  \n",
    "1. Construct a **query vector** for each search term (“markov chains” and “facebook”) using the same TF-IDF weighting as used for the documents.  \n",
    "2. Compute **cosine similarity** between the query vector and every document vector from the TF-IDF matrix \\( X \\) :  \n",
    "   $$\n",
    "   \\text{sim}(d_i, d_j) = \\frac{d_i^T d_j}{\\|d_i\\| \\|d_j\\|}\n",
    "   $$\n",
    "3. Rank the documents based on their similarity scores, and output the top 5 results for each query.\n",
    "\n",
    "### Intuition and Interpretation of the Results :\n",
    "- **For the query “markov chains”** :  \n",
    "  The top results consist of courses directly related to stochastic processes and Markov chains (like, “Applied probability & stochastic processes” and “Markov chains and algorithmic applications”). This is expected because these courses contain the query terms or closely related terminology. The cosine similarity scores reflect the overlap between the query vector and the course vectors in the TF-IDF space.\n",
    "\n",
    "- **For the query “facebook”** :  \n",
    "  The top result is “Computational Social Media” with a low similarity score (0.1836), which makes sense because it likely mentions Facebook explicitly or discusses related topics like social networks. The remaining courses have a **similarity score of 0.0000**, indicating that they do not contain the term “facebook” (or any similar token) in their descriptions.  \n",
    "  This occurs because the **query vector** only has nonzero values for terms like “facebook”, and if a document vector lacks these terms entirely, the dot product with the query vector is zero, leading to a cosine similarity of zero.\n",
    "\n",
    "- **Why does this happen?**  \n",
    "  Cosine similarity measures how well the directions of two vectors align. If a document contains none of the terms in the query, the vectors are orthogonal (dot product = 0), resulting in a similarity of 0. In contrast, courses with relevant content (like “markov chains”) have vectors with overlapping terms and thus higher similarity scores.\n",
    "\n",
    "- **Possible improvements** :  \n",
    "  - Incorporating **synonyms or semantically related terms** could improve retrieval. For example, “social network” might be relevant for “facebook”.  \n",
    "  - Using **word embeddings (like, Word2Vec)** can capture semantic relationships even if exact terms don’t overlap.\n",
    "\n",
    "### Conclusion :\n",
    "This experiment demonstrates how a **vector space model with TF-IDF** can be used to perform document similarity searches. The method effectively highlights courses closely related to the query terms but struggles when no direct term overlap exists.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
