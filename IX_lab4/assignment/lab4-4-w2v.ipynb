{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Kenza Driss*\n",
    "* *Maximilien Hoffbeck*\n",
    "* *Jaeyi Jeong*\n",
    "* *Yoojin Kim*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 854 course descriptions processed.\n",
      " Vocabulary size (case-sensitive): 19081\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "course_names = []\n",
    "course_descriptions = []\n",
    "\n",
    "with open(\"data/courses.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        course_names.append(data[\"name\"])\n",
    "        course_descriptions.append(data[\"description\"])\n",
    "\n",
    "\n",
    "with open(\"data/stopwords.pkl\", \"rb\") as f:\n",
    "    stopwords = pickle.load(f)\n",
    "\n",
    "\n",
    "tokenized_courses = []\n",
    "vocab = set()\n",
    "\n",
    "for desc in course_descriptions:\n",
    "    tokens = re.findall(r\"\\b[A-Za-z0-9]+\\b\", desc)\n",
    "    tokens = [t for t in tokens if t not in stopwords]  \n",
    "    tokenized_courses.append(tokens)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "print(f\" {len(course_descriptions)} course descriptions processed.\")\n",
    "print(f\" Vocabulary size (case-sensitive): {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13300 words in your vocabulary are in the pretrained Word2Vec model.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "w2v_path = \"/home/ix/ix-data/model.txt\"\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=False)\n",
    "\n",
    "\n",
    "words_in_model = [w for w in vocab if w in w2v]\n",
    "\n",
    "print(f\" {len(words_in_model)} words in your vocabulary are in the pretrained Word2Vec model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cluster 0\n",
      "  thin\n",
      "  casing\n",
      "  cylindrical\n",
      "  sliding\n",
      "  wires\n",
      "  vertical\n",
      "  stacked\n",
      "  horizontal\n",
      "  thick\n",
      "  mesh\n",
      "\n",
      " Cluster 1\n",
      "  notion\n",
      "  context\n",
      "  concepts\n",
      "  notions\n",
      "  normative\n",
      "  epistemologies\n",
      "  interpretation\n",
      "  understanding\n",
      "  implications\n",
      "  assumptions\n",
      "\n",
      " Cluster 2\n",
      "  photodetectors\n",
      "  photocurrent\n",
      "  nonlinearities\n",
      "  photoluminescence\n",
      "  photomultipliers\n",
      "  photoemission\n",
      "  dielectric\n",
      "  excitation\n",
      "  linewidth\n",
      "  electromigration\n",
      "\n",
      " Cluster 3\n",
      "  Biochemistry\n",
      "  Microbiology\n",
      "  Neuroscience\n",
      "  Physics\n",
      "  Biomedical\n",
      "  Chemistry\n",
      "  Psychology\n",
      "  Neurophysiology\n",
      "  Biophysics\n",
      "  Applied\n",
      "\n",
      " Cluster 4\n",
      "  quadratic\n",
      "  ODEs\n",
      "  eigenvalues\n",
      "  summability\n",
      "  invariant\n",
      "  discretized\n",
      "  finite\n",
      "  equations\n",
      "  stationarity\n",
      "  integrability\n",
      "\n",
      " Cluster 5\n",
      "  investments\n",
      "  investment\n",
      "  enterprises\n",
      "  suppliers\n",
      "  investing\n",
      "  procurement\n",
      "  healthcare\n",
      "  outsourcing\n",
      "  pricing\n",
      "  enterprise\n",
      "\n",
      " Cluster 6\n",
      "  texts\n",
      "  illustrations\n",
      "  essays\n",
      "  illustrative\n",
      "  illustrating\n",
      "  inspired\n",
      "  references\n",
      "  illustrated\n",
      "  compositions\n",
      "  formes\n",
      "\n",
      " Cluster 7\n",
      "  knowing\n",
      "  understand\n",
      "  understands\n",
      "  thinking\n",
      "  feeling\n",
      "  imagine\n",
      "  realize\n",
      "  comprehend\n",
      "  mind\n",
      "  empathy\n",
      "\n",
      " Cluster 8\n",
      "  wetlands\n",
      "  vegetation\n",
      "  topography\n",
      "  groundwater\n",
      "  basin\n",
      "  reservoirs\n",
      "  sediment\n",
      "  soils\n",
      "  biotopes\n",
      "  ecosystems\n",
      "\n",
      " Cluster 9\n",
      "  167\n",
      "  232\n",
      "  310\n",
      "  238\n",
      "  213\n",
      "  136\n",
      "  137\n",
      "  210\n",
      "  261\n",
      "  252\n",
      "\n",
      " Cluster 10\n",
      "  optimal\n",
      "  optimizing\n",
      "  utilization\n",
      "  reduce\n",
      "  minimize\n",
      "  optimum\n",
      "  increases\n",
      "  affect\n",
      "  Therefore\n",
      "  evaluating\n",
      "\n",
      " Cluster 11\n",
      "  apoptosis\n",
      "  misfolding\n",
      "  neuronal\n",
      "  hematopoiesis\n",
      "  inhibition\n",
      "  proteins\n",
      "  intracellular\n",
      "  neuroprotection\n",
      "  neuroendocrine\n",
      "  epithelial\n",
      "\n",
      " Cluster 12\n",
      "  Implementation\n",
      "  Analysis\n",
      "  Policies\n",
      "  Methodology\n",
      "  Evaluation\n",
      "  Context\n",
      "  Methodologies\n",
      "  Budgeting\n",
      "  Assessment\n",
      "  Transformation\n",
      "\n",
      " Cluster 13\n",
      "  Measurement\n",
      "  Resistive\n",
      "  Optimization\n",
      "  Perturbation\n",
      "  Multiscale\n",
      "  Applications\n",
      "  Kinematics\n",
      "  Reactivity\n",
      "  Impedance\n",
      "  Polymeric\n",
      "\n",
      " Cluster 14\n",
      "  informed\n",
      "  intervene\n",
      "  inform\n",
      "  oppose\n",
      "  request\n",
      "  requested\n",
      "  seek\n",
      "  seeking\n",
      "  enforce\n",
      "  intervention\n",
      "\n",
      " Cluster 15\n",
      "  Make\n",
      "  Mind\n",
      "  Know\n",
      "  Shape\n",
      "  Perfect\n",
      "  Us\n",
      "  Hands\n",
      "  Take\n",
      "  Noise\n",
      "  Trouble\n",
      "\n",
      " Cluster 16\n",
      "  software\n",
      "  interfaces\n",
      "  implementations\n",
      "  APIs\n",
      "  functionality\n",
      "  interface\n",
      "  Simulink\n",
      "  GUI\n",
      "  OLTP\n",
      "  LabVIEW\n",
      "\n",
      " Cluster 17\n",
      "  polyol\n",
      "  carbonylation\n",
      "  silane\n",
      "  polymers\n",
      "  compounds\n",
      "  azides\n",
      "  thiols\n",
      "  nitrides\n",
      "  polymeric\n",
      "  hydrolysis\n",
      "\n",
      " Cluster 18\n",
      "  2009\n",
      "  2007\n",
      "  2003\n",
      "  2008\n",
      "  2005\n",
      "  1999\n",
      "  2012\n",
      "  1998\n",
      "  2013\n",
      "  2004\n",
      "\n",
      " Cluster 19\n",
      "  Thompson\n",
      "  Moore\n",
      "  Baker\n",
      "  Smith\n",
      "  Patterson\n",
      "  Robinson\n",
      "  Taylor\n",
      "  Freeman\n",
      "  Allen\n",
      "  Richardson\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([w2v[word] for word in words_in_model])\n",
    "\n",
    "\n",
    "X_normalized = normalize(X)\n",
    "\n",
    "\n",
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "kmeans.fit(X_normalized)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"\\n Cluster {i}\")\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "    cluster_words = [words_in_model[j] for j in cluster_indices]\n",
    "    cluster_vectors = X_normalized[cluster_indices]\n",
    "    \n",
    "    sims = cosine_similarity(cluster_vectors, centroids[i].reshape(1, -1)).flatten()\n",
    "    top_indices = sims.argsort()[-10:][::-1]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        print(f\"  {cluster_words[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Top-10 words per cluster\n",
    "\n",
    "See output above. We clustered the Word2Vec vectors of **13,300 vocabulary words that were present in the pretrained model** into **k = 20** clusters using KMeans after normalization (so that cosine similarity ≈ Euclidean distance). For each cluster, we printed the 10 most similar words to the centroid.\n",
    "\n",
    "#### 2. Cluster types and interpretation\n",
    "\n",
    "Here are 10 example clusters and their interpreted theme:\n",
    "\n",
    "- **Cluster 0 – Mechanical structures / physical components**: `thin`, `casing`, `cylindrical`, `sliding`, `mesh`...\n",
    "- **Cluster 1 – Abstract reasoning / epistemology**: `notion`, `context`, `assumptions`, `epistemologies`, `interpretation`...\n",
    "- **Cluster 2 – Photonics / optoelectronics**: `photodetectors`, `photocurrent`, `photoluminescence`, `excitation`, `dielectric`...\n",
    "- **Cluster 3 – Scientific disciplines**: `Biochemistry`, `Neuroscience`, `Psychology`, `Physics`, `Biomedical`...\n",
    "- **Cluster 4 – Applied mathematics / equations**: `ODEs`, `quadratic`, `eigenvalues`, `stationarity`, `discretized`...\n",
    "- **Cluster 5 – Economics / supply chain**: `investments`, `suppliers`, `procurement`, `enterprise`, `pricing`...\n",
    "- **Cluster 6 – Visual communication / illustration**: `illustrations`, `essays`, `compositions`, `references`, `formes`...\n",
    "- **Cluster 7 – Human cognition / understanding**: `knowing`, `thinking`, `comprehend`, `mind`, `empathy`...\n",
    "- **Cluster 8 – Environmental science / ecosystems**: `wetlands`, `vegetation`, `groundwater`, `basin`, `biotopes`...\n",
    "- **Cluster 11 – Cell biology / neuroscience**: `apoptosis`, `neuronal`, `proteins`, `neuroprotection`, `inhibition`...\n",
    "\n",
    "Some clusters (exp Cluster 9 or 19) are ambiguous or noisy, including numbers or surnames, but most are semantically coherent.\n",
    "\n",
    "#### 3. Comparison to LSI and LDA\n",
    "\n",
    "- **LSI** and **LDA** group terms based on their usage patterns across documents (topics as co-occurrence patterns).\n",
    "- **KMeans on Word2Vec** groups terms based on **semantic similarity in pretrained embeddings** (from Wikipedia), regardless of our corpus.\n",
    "\n",
    "Compared to LSI/LDA:\n",
    "- These clusters are **sharper and cleaner** (exp Cluster 2 = clearly photonics-related).\n",
    "- But they lack **document-level context**, it's term-only.\n",
    "- However, many LSI/LDA topics align closely with the clusters, showing that Word2Vec embeddings are **consistent with latent topics** discovered from the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Markov chains'\n",
      "\n",
      "Score: 0.6352 | Applied stochastic processes: This course introduces the theory of stochastic processes including Markov chain...\n",
      "Score: 0.5740 | Applied probability & stochastic processes: This course focuses on dynamic models of random phenomena, and in particular, th...\n",
      "Score: 0.5652 | Markov chains and algorithmic applications: The study of random walks finds many applications in computer science and commun...\n",
      "Score: 0.5611 | Statistical Sequence Processing: This course discusses advanced methods extensively used for the processing, pred...\n",
      "Score: 0.5343 | Polymer chemistry and macromolecular engineering: Know modern methods of polymer synthesis. Understand how parameters, which deter...\n",
      "\n",
      "Query: 'Facebook'\n",
      "\n",
      "Score: 0.6106 | Computational Social Media: The course integrates concepts from media studies, machine learning, multimedia ...\n",
      "Score: 0.4408 | Computer networks: This course provides an introduction to computer networks. It describes the prin...\n",
      "Score: 0.4366 | Privacy Protection: Main threats against privacy, description of protection techniques and of their ...\n",
      "Score: 0.4288 | Internet analytics: Internet analytics is the collection, modeling, and analysis of user data in lar...\n",
      "Score: 0.4283 | Media security: This course provides attendees with theoretical and practical issues in media se...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "corpus_joined = [\" \".join(doc) for doc in tokenized_courses]\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=words_in_model, lowercase=False)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_joined)\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "\n",
    "def document_vector(doc_tokens):\n",
    "    vecs = []\n",
    "    for word in doc_tokens:\n",
    "        if word in w2v and word in idf_dict:\n",
    "            weight = idf_dict[word]\n",
    "            vecs.append(weight * w2v[word])\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(w2v.vector_size)\n",
    "\n",
    "doc_vectors = np.array([document_vector(doc) for doc in tokenized_courses])\n",
    "doc_vectors = normalize(doc_vectors)\n",
    "\n",
    "def search(query, top_k=5):\n",
    "    tokens = re.findall(r\"\\b[A-Za-z0-9]+\\b\", query)\n",
    "    tokens = [t for t in tokens if t in w2v and t in idf_dict]\n",
    "    \n",
    "    if not tokens:\n",
    "        print(f\"No valid words found in query '{query}'\")\n",
    "        return\n",
    "\n",
    "    query_vec = document_vector(tokens).reshape(1, -1)\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    sims = cosine_similarity(doc_vectors, query_vec).flatten()\n",
    "    top_indices = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "    print(f\"\\nQuery: '{query}'\\n\")\n",
    "    for i in top_indices:\n",
    "        print(f\"Score: {sims[i]:.4f} | {course_names[i]}: {course_descriptions[i][:80]}...\")\n",
    "\n",
    "search(\"Markov chains\")\n",
    "search(\"Facebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We implemented a search function where each course is represented as a **TF-IDF–weighted average of Word2Vec word vectors**.  \n",
    "Queries are processed in the same way, and we use **cosine similarity** to compare them to each course.\n",
    "\n",
    "#### Results for \"Markov chains\":\n",
    "\n",
    "The top-ranked courses are highly relevant:\n",
    "- **Applied stochastic processes**\n",
    "- **Applied probability & stochastic processes**\n",
    "- **Markov chains and algorithmic applications**\n",
    "- **Statistical Sequence Processing**\n",
    "\n",
    "All of these explicitly focus on stochastic processes, random walks, or statistical modeling, confirming that the model successfully retrieves semantically appropriate results, even when the exact query words are not repeated.\n",
    "\n",
    "#### Results for \"Facebook\":\n",
    "\n",
    "The highest-ranked courses include:\n",
    "- **Computational Social Media**\n",
    "- **Computer networks**\n",
    "- **Privacy Protection**\n",
    "- **Internet analytics**\n",
    "- **Media security**\n",
    "\n",
    "These courses deal with social networks, online communication, privacy, and media, all thematically related to Facebook. Even though the word “Facebook” might not appear in most descriptions, the system generalizes well through word embeddings.\n",
    "\n",
    "#### Comparison with Vector Space Model and LSI:\n",
    "\n",
    "Compared to the **vector space model** (TF-IDF only), this method captures **semantic similarity** rather than relying on exact keyword matches. In contrast, TF-IDF would miss several relevant courses due to vocabulary mismatch.\n",
    "\n",
    "Compared to **LSI**, Word2Vec leverages pretrained semantic knowledge at the word level (from Wikipedia), allowing more fine-grained matching. LSI focuses on latent topics at the document level, while Word2Vec provides a more **context-aware and locally precise** representation.\n",
    "\n",
    "Overall, Word2Vec + TF-IDF offers **more robust, flexible, and meaningful document matching**, especially for short or expressive queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"MySpace\" in vocab)\n",
    "print(\"Orkut\" in vocab)\n",
    "print(\"coronavirus\" in vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'MySpace Orkut' (words not in corpus)\n",
      "\n",
      "Score: 0.5942 | Computational Social Media: The course integrates concepts from media studies, machine learning, multimedia ...\n",
      "Score: 0.4841 | Computer networks: This course provides an introduction to computer networks. It describes the prin...\n",
      "Score: 0.4520 | Mobile networks: This course provides a detailed description of the organization and operating pr...\n",
      "Score: 0.4479 | Privacy Protection: Main threats against privacy, description of protection techniques and of their ...\n",
      "Score: 0.4454 | Internet analytics: Internet analytics is the collection, modeling, and analysis of user data in lar...\n",
      "\n",
      "Query: 'coronavirus' (words not in corpus)\n",
      "\n",
      "Score: 0.5193 | Infection biology: Infectious diseases (ID) are still a major problem to human health. But how do p...\n",
      "Score: 0.5086 | Biotechnology lab (for CGC): Students apply basic techniques in molecular biology to clone a cDNA of interest...\n",
      "Score: 0.5067 | Practical - Lemaitre Lab: Drosophila immunity. Give students a feel for some of the approaches pursued to ...\n",
      "Score: 0.5039 | Landmark Papers in Cancer and Infection: The topics will cover all the fields in which the GHI and ISREC PIs work. Conten...\n",
      "Score: 0.4966 | Gene transfer and recombinant protein expression in animal cells: Recombinant proteins synthesized by animal cells are becoming increasingly impor...\n"
     ]
    }
   ],
   "source": [
    "def search_generalized(query, top_k=5):\n",
    "    tokens = re.findall(r\"\\b[A-Za-z0-9]+\\b\", query)\n",
    "    tokens = [t for t in tokens if t in w2v]\n",
    "\n",
    "    if not tokens:\n",
    "        print(f\"No recognized words in query '{query}' (not in Word2Vec vocabulary).\")\n",
    "        return\n",
    "\n",
    "    vecs = [w2v[word] for word in tokens]\n",
    "    query_vec = np.mean(vecs, axis=0).reshape(1, -1)\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    sims = cosine_similarity(doc_vectors, query_vec).flatten()\n",
    "    top_indices = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "    print(f\"\\nQuery: '{query}' (words not in corpus)\\n\")\n",
    "    for i in top_indices:\n",
    "        print(f\"Score: {sims[i]:.4f} | {course_names[i]}: {course_descriptions[i][:80]}...\")\n",
    "\n",
    "search_generalized(\"MySpace Orkut\")\n",
    "search_generalized(\"coronavirus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We tested the ability of our Word2Vec-based search system to generalize to queries containing words that do not appear in our dataset. Thanks to the pretrained embeddings, which capture semantic information from large corpora such as Wikipedia, the model can still retrieve relevant documents by leveraging similarity in meaning, even when the exact query terms are unseen in the corpus.\n",
    "\n",
    "#### Query: \"MySpace Orkut\"\n",
    "\n",
    "Neither \"MySpace\" nor \"Orkut\" appears in any EPFL course description. However, the top retrieved courses included:\n",
    "- Computational Social Media\n",
    "- Computer Networks\n",
    "- Privacy Protection\n",
    "- Internet Analytics\n",
    "- Media Security\n",
    "\n",
    "These courses are thematically related to online platforms, digital communication, and social media, demonstrating that the system can generalize well to related topics using only pretrained word semantics. The results are also comparable to what we obtained for the query \"Facebook\" in Exercise 4.13, confirming the consistency of semantic matching.\n",
    "\n",
    "#### Query: \"coronavirus\"\n",
    "\n",
    "The term \"coronavirus\" is not present in the course corpus. Nonetheless, the system retrieved:\n",
    "- Infectious Diseases\n",
    "- Molecular Biology Lab\n",
    "- Drosophila Immunity\n",
    "- Biomedical Research\n",
    "- Protein Engineering\n",
    "\n",
    "These courses are directly relevant to biology, immunology, and public health, and would be suitable for someone interested in the scientific context of a pandemic. This shows that the model can link the term \"coronavirus\" to courses covering related biological and health-related content, a task that traditional TF-IDF or LSI models would fail at due to vocabulary mismatch.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "This experiment highlights the advantage of using pretrained word embeddings: they provide strong generalization capabilities by capturing the semantic relationships between words beyond the training corpus. Unlike LSI or traditional vector space models, this allows our system to handle queries that include terms entirely missing from the document set while still producing meaningful and accurate results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
